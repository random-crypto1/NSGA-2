{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "heHWTGtMwBDu",
        "outputId": "76a18093-98fc-4b71-87f6-8eb8d0dfe327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2770072040.py:12: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'superconductor-dataset' dataset.\n",
            "First 5 records:    number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass  \\\n",
            "0                   4         88.944468             57.862692   \n",
            "1                   5         92.729214             58.518416   \n",
            "2                   4         88.944468             57.885242   \n",
            "3                   4         88.944468             57.873967   \n",
            "4                   4         88.944468             57.840143   \n",
            "\n",
            "   gmean_atomic_mass  wtd_gmean_atomic_mass  entropy_atomic_mass  \\\n",
            "0          66.361592              36.116612             1.181795   \n",
            "1          73.132787              36.396602             1.449309   \n",
            "2          66.361592              36.122509             1.181795   \n",
            "3          66.361592              36.119560             1.181795   \n",
            "4          66.361592              36.110716             1.181795   \n",
            "\n",
            "   wtd_entropy_atomic_mass  range_atomic_mass  wtd_range_atomic_mass  \\\n",
            "0                 1.062396          122.90607              31.794921   \n",
            "1                 1.057755          122.90607              36.161939   \n",
            "2                 0.975980          122.90607              35.741099   \n",
            "3                 1.022291          122.90607              33.768010   \n",
            "4                 1.129224          122.90607              27.848743   \n",
            "\n",
            "   std_atomic_mass  ...  wtd_mean_Valence  gmean_Valence  wtd_gmean_Valence  \\\n",
            "0        51.968828  ...          2.257143       2.213364           2.219783   \n",
            "1        47.094633  ...          2.257143       1.888175           2.210679   \n",
            "2        51.968828  ...          2.271429       2.213364           2.232679   \n",
            "3        51.968828  ...          2.264286       2.213364           2.226222   \n",
            "4        51.968828  ...          2.242857       2.213364           2.206963   \n",
            "\n",
            "   entropy_Valence  wtd_entropy_Valence  range_Valence  wtd_range_Valence  \\\n",
            "0         1.368922             1.066221              1           1.085714   \n",
            "1         1.557113             1.047221              2           1.128571   \n",
            "2         1.368922             1.029175              1           1.114286   \n",
            "3         1.368922             1.048834              1           1.100000   \n",
            "4         1.368922             1.096052              1           1.057143   \n",
            "\n",
            "   std_Valence  wtd_std_Valence  critical_temp  \n",
            "0     0.433013         0.437059           29.0  \n",
            "1     0.632456         0.468606           26.0  \n",
            "2     0.433013         0.444697           19.0  \n",
            "3     0.433013         0.440952           22.0  \n",
            "4     0.433013         0.428809           23.0  \n",
            "\n",
            "[5 rows x 82 columns]\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies as needed:\n",
        "# pip install kagglehub[pandas-datasets]\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# Set the path to the file you'd like to load\n",
        "# You need to specify a file name from the dataset, e.g., 'superconductor_data.csv'\n",
        "# You can check the dataset page on Kaggle for available files.\n",
        "file_path = \"train.csv\"\n",
        "\n",
        "# Load the latest version\n",
        "df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"munumbutt/superconductor-dataset\",\n",
        "  file_path,\n",
        "  # Provide any additional arguments like\n",
        "  # sql_query or pandas_kwargs. See the\n",
        "  # documenation for more information:\n",
        "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
        ")\n",
        "\n",
        "print(\"First 5 records:\", df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparation"
      ],
      "metadata": {
        "id": "_kSBV6svC3Xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "from dataclasses import dataclass\n"
      ],
      "metadata": {
        "id": "l1VWRjiRxZkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 2) Split features / label\n",
        "# =========================\n",
        "# Most common label column name for this dataset is \"critical_temp\"\n",
        "# If your label column differs, update it here.\n",
        "target_col = \"critical_temp\"\n",
        "assert target_col in df.columns, f\"Target column '{target_col}' not found. Columns: {df.columns.tolist()}\"\n",
        "\n",
        "X = df.drop(columns=[target_col]).values.astype(np.float32)\n",
        "y = df[target_col].values.astype(np.float32).reshape(-1, 1)\n",
        "\n",
        "print(\"X shape:\", X.shape, \"| y shape:\", y.shape)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 3) Train / Val / Test split\n",
        "# =========================\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 4) Standardize (fit on train only!)\n",
        "# =========================\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
        "X_val = scaler.transform(X_val).astype(np.float32)\n",
        "X_test = scaler.transform(X_test).astype(np.float32)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 5) Build PyTorch DataLoaders\n",
        "# =========================\n",
        "batch_size = 256\n",
        "\n",
        "train_ds = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
        "val_ds = TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n",
        "test_ds = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJsPb5JkBYhe",
        "outputId": "180d5a0f-fea2-41e9-e6a8-a1131e15e40c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (21263, 81) | y shape: (21263, 1)\n",
            "Train: (17010, 81) Val: (2126, 81) Test: (2127, 81)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. NSGA-2\n",
        "\n"
      ],
      "metadata": {
        "id": "t3UN5x2LBc9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =========================\n",
        "# 1) Chromosome Definition\n",
        "# =========================\n",
        "ACTIVATIONS = [\"relu\", \"gelu\", \"tanh\", \"leaky_relu\"]\n",
        "\n",
        "@dataclass\n",
        "class Chromosome:\n",
        "    # Feature selection\n",
        "    feature_mask: np.ndarray  # shape (81,), dtype=bool or int {0,1}\n",
        "\n",
        "    # Architecture\n",
        "    num_layers: int           # 1..4\n",
        "    hidden_units: np.ndarray  # shape (4,), only first num_layers used\n",
        "    activation: str           # in ACTIVATIONS\n",
        "\n",
        "    # Optional (nice for performance)\n",
        "    dropout: float = 0.1\n",
        "    lr: float = 1e-3\n",
        "    weight_decay: float = 1e-5\n",
        "\n",
        "\n",
        "def random_chromosome(n_features=81) -> Chromosome:\n",
        "    # feature mask: keep ~10-40 features initially (prevents degenerate too-sparse)\n",
        "    mask = np.zeros(n_features, dtype=np.int32)\n",
        "    k = np.random.randint(8, 41)\n",
        "    idx = np.random.choice(n_features, size=k, replace=False)\n",
        "    mask[idx] = 1\n",
        "\n",
        "    num_layers = np.random.randint(1, 5)  # 1..4\n",
        "    hidden_units = np.random.randint(16, 513, size=(4,))  # 16..512\n",
        "\n",
        "    activation = np.random.choice(ACTIVATIONS)\n",
        "\n",
        "    dropout = np.random.uniform(0.0, 0.4)\n",
        "    lr = 10 ** np.random.uniform(-4, -3)  # 1e-4 to 1e-3\n",
        "    wd = np.random.uniform(0.0, 1e-3)\n",
        "\n",
        "    return Chromosome(\n",
        "        feature_mask=mask,\n",
        "        num_layers=num_layers,\n",
        "        hidden_units=hidden_units,\n",
        "        activation=activation,\n",
        "        dropout=float(dropout),\n",
        "        lr=float(lr),\n",
        "        weight_decay=float(wd),\n",
        "    )\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 2) Decode -> Build PyTorch model\n",
        "# =========================\n",
        "def get_activation(name: str):\n",
        "    if name == \"relu\":\n",
        "        return nn.ReLU()\n",
        "    if name == \"gelu\":\n",
        "        return nn.GELU()\n",
        "    if name == \"tanh\":\n",
        "        return nn.Tanh()\n",
        "    if name == \"leaky_relu\":\n",
        "        return nn.LeakyReLU(0.1)\n",
        "    raise ValueError(f\"Unknown activation: {name}\")\n",
        "\n",
        "\n",
        "class EvoMLP(nn.Module):\n",
        "    def __init__(self, input_dim, num_layers, hidden_units, activation, dropout):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev = input_dim\n",
        "        act = get_activation(activation)\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            h = int(hidden_units[i])\n",
        "            layers.append(nn.Linear(prev, h))\n",
        "            layers.append(act)\n",
        "            if dropout > 0:\n",
        "                layers.append(nn.Dropout(dropout))\n",
        "            prev = h\n",
        "\n",
        "        layers.append(nn.Linear(prev, 1))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def count_params(model: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 3) Evaluate Chromosome -> 3 objectives\n",
        "#    f1: RMSE (val)\n",
        "#    f2: #params\n",
        "#    f3: #selected features\n",
        "# =========================\n",
        "def evaluate_chromosome(\n",
        "    chrom: Chromosome,\n",
        "    X_train, y_train, X_val, y_val,\n",
        "    epochs=30,\n",
        "    batch_size=256,\n",
        "    device=None,\n",
        "    min_features=5\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns: (rmse, n_params, n_features_selected)\n",
        "    All minimized.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # --- Feature mask ---\n",
        "    mask = chrom.feature_mask.astype(bool)\n",
        "    n_selected = int(mask.sum())\n",
        "\n",
        "    # Constraint: avoid too few features\n",
        "    if n_selected < min_features:\n",
        "        # Huge penalty so NSGA-II will avoid it\n",
        "        return (1e9, 1e9, n_selected)\n",
        "\n",
        "    Xtr = X_train[:, mask]\n",
        "    Xva = X_val[:, mask]\n",
        "\n",
        "    # Torch tensors\n",
        "    Xtr_t = torch.tensor(Xtr, dtype=torch.float32)\n",
        "    ytr_t = torch.tensor(y_train, dtype=torch.float32)\n",
        "    Xva_t = torch.tensor(Xva, dtype=torch.float32)\n",
        "    yva_t = torch.tensor(y_val, dtype=torch.float32)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.TensorDataset(Xtr_t, ytr_t),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    model = EvoMLP(\n",
        "        input_dim=Xtr.shape[1],\n",
        "        num_layers=chrom.num_layers,\n",
        "        hidden_units=chrom.hidden_units,\n",
        "        activation=chrom.activation,\n",
        "        dropout=chrom.dropout,\n",
        "    ).to(device)\n",
        "\n",
        "    n_params = count_params(model)\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=chrom.lr,\n",
        "        weight_decay=chrom.weight_decay,\n",
        "    )\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    # --- training (fixed small budget) ---\n",
        "    model.train()\n",
        "    for _ in range(epochs):\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(xb)\n",
        "            loss = loss_fn(pred, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # --- validation RMSE ---\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = model(Xva_t.to(device)).cpu().numpy().reshape(-1)\n",
        "        trues = yva_t.cpu().numpy().reshape(-1)\n",
        "        rmse = float(np.sqrt(mean_squared_error(trues, preds)))\n",
        "\n",
        "    return (rmse, n_params, n_selected)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 4) Genetic Operators\n",
        "# =========================\n",
        "def uniform_crossover(mask1, mask2, p=0.5):\n",
        "    \"\"\"Bitwise uniform crossover\"\"\"\n",
        "    assert mask1.shape == mask2.shape\n",
        "    swap = np.random.rand(mask1.shape[0]) < p\n",
        "    child1 = mask1.copy()\n",
        "    child2 = mask2.copy()\n",
        "    child1[swap], child2[swap] = child2[swap], child1[swap]\n",
        "    return child1, child2\n",
        "\n",
        "\n",
        "def crossover(parent1: Chromosome, parent2: Chromosome) -> tuple[Chromosome, Chromosome]:\n",
        "    # Feature mask crossover\n",
        "    m1, m2 = uniform_crossover(parent1.feature_mask, parent2.feature_mask, p=0.5)\n",
        "\n",
        "    # num_layers crossover (pick one)\n",
        "    L1 = parent1.num_layers if np.random.rand() < 0.5 else parent2.num_layers\n",
        "    L2 = parent2.num_layers if np.random.rand() < 0.5 else parent1.num_layers\n",
        "\n",
        "    # hidden units crossover (per-gene)\n",
        "    hu1 = parent1.hidden_units.copy()\n",
        "    hu2 = parent2.hidden_units.copy()\n",
        "    for i in range(4):\n",
        "        if np.random.rand() < 0.5:\n",
        "            hu1[i], hu2[i] = hu2[i], hu1[i]\n",
        "\n",
        "    # activation crossover\n",
        "    a1 = parent1.activation if np.random.rand() < 0.5 else parent2.activation\n",
        "    a2 = parent2.activation if np.random.rand() < 0.5 else parent1.activation\n",
        "\n",
        "    # dropout/lr/wd crossover (simple averaging)\n",
        "    d1 = float((parent1.dropout + parent2.dropout) / 2.0)\n",
        "    d2 = d1\n",
        "    lr1 = float((parent1.lr + parent2.lr) / 2.0)\n",
        "    lr2 = lr1\n",
        "    wd1 = float((parent1.weight_decay + parent2.weight_decay) / 2.0)\n",
        "    wd2 = wd1\n",
        "\n",
        "    return (\n",
        "        Chromosome(m1, L1, hu1, a1, d1, lr1, wd1),\n",
        "        Chromosome(m2, L2, hu2, a2, d2, lr2, wd2),\n",
        "    )\n",
        "\n",
        "\n",
        "def mutate(chrom: Chromosome, p_mask=0.03, p_arch=0.2, n_features=81) -> Chromosome:\n",
        "    c = Chromosome(\n",
        "        feature_mask=chrom.feature_mask.copy(),\n",
        "        num_layers=int(chrom.num_layers),\n",
        "        hidden_units=chrom.hidden_units.copy(),\n",
        "        activation=str(chrom.activation),\n",
        "        dropout=float(chrom.dropout),\n",
        "        lr=float(chrom.lr),\n",
        "        weight_decay=float(chrom.weight_decay),\n",
        "    )\n",
        "\n",
        "    # --- Feature mask mutation (bit flip) ---\n",
        "    # flip each bit with small prob\n",
        "    flip = np.random.rand(n_features) < (p_mask / n_features * 81)  # scaled\n",
        "    c.feature_mask[flip] = 1 - c.feature_mask[flip]\n",
        "\n",
        "    # keep at least 1 feature alive (hard safety)\n",
        "    if c.feature_mask.sum() == 0:\n",
        "        c.feature_mask[np.random.randint(0, n_features)] = 1\n",
        "\n",
        "    # --- Architecture mutation ---\n",
        "    if np.random.rand() < p_arch:\n",
        "        # mutate num_layers\n",
        "        if np.random.rand() < 0.5:\n",
        "            c.num_layers = int(np.clip(c.num_layers + np.random.choice([-1, 1]), 1, 4))\n",
        "\n",
        "        # mutate hidden units (random reset per layer)\n",
        "        for i in range(4):\n",
        "            if np.random.rand() < 0.3:\n",
        "                c.hidden_units[i] = np.random.randint(16, 513)\n",
        "\n",
        "        # mutate activation\n",
        "        if np.random.rand() < 0.2:\n",
        "            c.activation = np.random.choice(ACTIVATIONS)\n",
        "\n",
        "        # mutate dropout / lr / wd\n",
        "        if np.random.rand() < 0.3:\n",
        "            c.dropout = float(np.clip(c.dropout + np.random.normal(0, 0.05), 0.0, 0.5))\n",
        "        if np.random.rand() < 0.3:\n",
        "            c.lr = float(np.clip(c.lr * (10 ** np.random.normal(0, 0.15)), 1e-4, 3e-3))\n",
        "        if np.random.rand() < 0.3:\n",
        "            c.weight_decay = float(np.clip(c.weight_decay + np.random.normal(0, 2e-4), 0.0, 1e-3))\n",
        "\n",
        "    return c\n"
      ],
      "metadata": {
        "id": "sGc-nOb7zuv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "# -------------------------\n",
        "# You already have these from earlier:\n",
        "# - Chromosome\n",
        "# - random_chromosome()\n",
        "# - crossover()\n",
        "# - mutate()\n",
        "# - evaluate_chromosome()\n",
        "# -------------------------\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1) NSGA-II Core Utilities\n",
        "# ============================================================\n",
        "def dominates(obj_a: Tuple[float, float, float], obj_b: Tuple[float, float, float]) -> bool:\n",
        "    \"\"\"\n",
        "    True if A dominates B (all <= and at least one <), for minimization.\n",
        "    \"\"\"\n",
        "    return (obj_a[0] <= obj_b[0] and obj_a[1] <= obj_b[1] and obj_a[2] <= obj_b[2]) and \\\n",
        "           (obj_a[0] <  obj_b[0] or  obj_a[1] <  obj_b[1] or  obj_a[2] <  obj_b[2])\n",
        "\n",
        "\n",
        "def fast_non_dominated_sort(objs: List[Tuple[float, float, float]]) -> List[List[int]]:\n",
        "    \"\"\"\n",
        "    Returns fronts as list of lists of indices.\n",
        "    NSGA-II fast non-dominated sorting.\n",
        "    \"\"\"\n",
        "    N = len(objs)\n",
        "    S = [[] for _ in range(N)]   # who i dominates\n",
        "    n = [0] * N                  # domination count\n",
        "    fronts = [[]]\n",
        "\n",
        "    for p in range(N):\n",
        "        for q in range(N):\n",
        "            if p == q:\n",
        "                continue\n",
        "            if dominates(objs[p], objs[q]):\n",
        "                S[p].append(q)\n",
        "            elif dominates(objs[q], objs[p]):\n",
        "                n[p] += 1\n",
        "\n",
        "        if n[p] == 0:\n",
        "            fronts[0].append(p)\n",
        "\n",
        "    i = 0\n",
        "    while len(fronts[i]) > 0:\n",
        "        next_front = []\n",
        "        for p in fronts[i]:\n",
        "            for q in S[p]:\n",
        "                n[q] -= 1\n",
        "                if n[q] == 0:\n",
        "                    next_front.append(q)\n",
        "        i += 1\n",
        "        fronts.append(next_front)\n",
        "\n",
        "    fronts.pop()  # last one empty\n",
        "    return fronts\n",
        "\n",
        "\n",
        "def crowding_distance(front: List[int], objs: List[Tuple[float, float, float]]) -> Dict[int, float]:\n",
        "    \"\"\"\n",
        "    Compute crowding distance for a front. Higher is better.\n",
        "    \"\"\"\n",
        "    dist = {idx: 0.0 for idx in front}\n",
        "    if len(front) <= 2:\n",
        "        for idx in front:\n",
        "            dist[idx] = float(\"inf\")\n",
        "        return dist\n",
        "\n",
        "    M = 3  # number of objectives\n",
        "    for m in range(M):\n",
        "        front_sorted = sorted(front, key=lambda i: objs[i][m])\n",
        "        dist[front_sorted[0]] = float(\"inf\")\n",
        "        dist[front_sorted[-1]] = float(\"inf\")\n",
        "\n",
        "        f_min = objs[front_sorted[0]][m]\n",
        "        f_max = objs[front_sorted[-1]][m]\n",
        "        if f_max == f_min:\n",
        "            continue\n",
        "\n",
        "        for k in range(1, len(front_sorted) - 1):\n",
        "            prev_i = front_sorted[k - 1]\n",
        "            next_i = front_sorted[k + 1]\n",
        "            dist[front_sorted[k]] += (objs[next_i][m] - objs[prev_i][m]) / (f_max - f_min)\n",
        "\n",
        "    return dist\n",
        "\n",
        "\n",
        "def tournament_select(\n",
        "    pop_indices: List[int],\n",
        "    rank: Dict[int, int],\n",
        "    crowd: Dict[int, float],\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Binary tournament selection:\n",
        "    - pick 2 random individuals\n",
        "    - choose lower rank\n",
        "    - if tie, choose higher crowding distance\n",
        "    \"\"\"\n",
        "    a, b = np.random.choice(pop_indices, 2, replace=False)\n",
        "    if rank[a] < rank[b]:\n",
        "        return a\n",
        "    if rank[b] < rank[a]:\n",
        "        return b\n",
        "    # same rank => crowding\n",
        "    return a if crowd[a] > crowd[b] else b\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2) Evaluation Cache (speed!)\n",
        "# ============================================================\n",
        "def chrom_key(chrom) -> Tuple:\n",
        "    \"\"\"\n",
        "    Hashable key for caching chromosome evaluation.\n",
        "    We round floats so tiny noise doesn't break caching.\n",
        "    \"\"\"\n",
        "    mask_bytes = chrom.feature_mask.astype(np.uint8).tobytes()\n",
        "    return (\n",
        "        mask_bytes,\n",
        "        int(chrom.num_layers),\n",
        "        tuple(int(x) for x in chrom.hidden_units.tolist()),\n",
        "        str(chrom.activation),\n",
        "        round(float(chrom.dropout), 4),\n",
        "        round(float(chrom.lr), 8),\n",
        "        round(float(chrom.weight_decay), 8),\n",
        "    )\n",
        "\n",
        "\n",
        "def evaluate_population(\n",
        "    population,\n",
        "    cache: Dict[Tuple, Tuple[float, float, float]],\n",
        "    X_train, y_train, X_val, y_val,\n",
        "    epochs=20,\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate all chromosomes, using cache.\n",
        "    Returns list of objectives aligned with population.\n",
        "    \"\"\"\n",
        "    objs = []\n",
        "    for chrom in population:\n",
        "        k = chrom_key(chrom)\n",
        "        if k not in cache:\n",
        "            cache[k] = evaluate_chromosome(\n",
        "                chrom, X_train, y_train, X_val, y_val,\n",
        "                epochs=epochs,\n",
        "                batch_size=256,\n",
        "                min_features=5,\n",
        "            )\n",
        "        objs.append(cache[k])\n",
        "    return objs\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3) Main NSGA-II Loop\n",
        "# ============================================================\n",
        "def nsga2_optimize(\n",
        "    X_train, y_train, X_val, y_val,\n",
        "    pop_size=60,\n",
        "    generations=30,\n",
        "    crossover_prob=0.9,\n",
        "    mutation_prob=0.9,\n",
        "    eval_epochs=20,\n",
        "    seed=42,\n",
        "    verbose=True,\n",
        "):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # --- init population ---\n",
        "    n_features = X_train.shape[1]\n",
        "    population = [random_chromosome(n_features=n_features) for _ in range(pop_size)]\n",
        "    cache = {}\n",
        "\n",
        "    # --- initial eval ---\n",
        "    objs = evaluate_population(population, cache, X_train, y_train, X_val, y_val, epochs=eval_epochs)\n",
        "\n",
        "    for gen in range(1, generations + 1):\n",
        "        # ----------------------------\n",
        "        # A) Non-dominated sorting\n",
        "        # ----------------------------\n",
        "        fronts = fast_non_dominated_sort(objs)\n",
        "\n",
        "        # rank and crowding distance dict for selection\n",
        "        rank = {}\n",
        "        crowd = {}\n",
        "        for r, front in enumerate(fronts):\n",
        "            for idx in front:\n",
        "                rank[idx] = r\n",
        "            cd = crowding_distance(front, objs)\n",
        "            crowd.update(cd)\n",
        "\n",
        "        # ----------------------------\n",
        "        # B) Generate offspring\n",
        "        # ----------------------------\n",
        "        pop_indices = list(range(pop_size))\n",
        "        offspring = []\n",
        "\n",
        "        while len(offspring) < pop_size:\n",
        "            p1_idx = tournament_select(pop_indices, rank, crowd)\n",
        "            p2_idx = tournament_select(pop_indices, rank, crowd)\n",
        "            parent1 = population[p1_idx]\n",
        "            parent2 = population[p2_idx]\n",
        "\n",
        "            # crossover\n",
        "            if np.random.rand() < crossover_prob:\n",
        "                c1, c2 = crossover(parent1, parent2)\n",
        "            else:\n",
        "                c1, c2 = parent1, parent2\n",
        "\n",
        "            # mutation\n",
        "            if np.random.rand() < mutation_prob:\n",
        "                c1 = mutate(c1, p_mask=0.05, p_arch=0.25, n_features=n_features)\n",
        "            if np.random.rand() < mutation_prob:\n",
        "                c2 = mutate(c2, p_mask=0.05, p_arch=0.25, n_features=n_features)\n",
        "\n",
        "            offspring.append(c1)\n",
        "            if len(offspring) < pop_size:\n",
        "                offspring.append(c2)\n",
        "\n",
        "        offspring_objs = evaluate_population(offspring, cache, X_train, y_train, X_val, y_val, epochs=eval_epochs)\n",
        "\n",
        "        # ----------------------------\n",
        "        # C) Elitist survival selection\n",
        "        # ----------------------------\n",
        "        combined_pop = population + offspring\n",
        "        combined_objs = objs + offspring_objs\n",
        "\n",
        "        combined_fronts = fast_non_dominated_sort(combined_objs)\n",
        "\n",
        "        new_population = []\n",
        "        new_objs = []\n",
        "\n",
        "        for front in combined_fronts:\n",
        "            if len(new_population) + len(front) <= pop_size:\n",
        "                for idx in front:\n",
        "                    new_population.append(combined_pop[idx])\n",
        "                    new_objs.append(combined_objs[idx])\n",
        "            else:\n",
        "                # partial fill using crowding distance\n",
        "                cd = crowding_distance(front, combined_objs)\n",
        "                sorted_front = sorted(front, key=lambda i: cd[i], reverse=True)\n",
        "\n",
        "                remaining = pop_size - len(new_population)\n",
        "                for idx in sorted_front[:remaining]:\n",
        "                    new_population.append(combined_pop[idx])\n",
        "                    new_objs.append(combined_objs[idx])\n",
        "                break\n",
        "\n",
        "        population = new_population\n",
        "        objs = new_objs\n",
        "\n",
        "        # ----------------------------\n",
        "        # D) Logging\n",
        "        # ----------------------------\n",
        "        # Pareto front = rank 0 in current population\n",
        "        current_fronts = fast_non_dominated_sort(objs)\n",
        "        pareto = current_fronts[0]\n",
        "\n",
        "        best_rmse = min(objs[i][0] for i in pareto)\n",
        "        best_sparse = min(objs[i][2] for i in pareto)\n",
        "        best_params = min(objs[i][1] for i in pareto)\n",
        "\n",
        "        if verbose:\n",
        "            print(\n",
        "                f\"Gen {gen:03d} | Pareto size={len(pareto)} | \"\n",
        "                f\"Best RMSE={best_rmse:.4f} | Min Feats={best_sparse} | Min Params={int(best_params)}\"\n",
        "            )\n",
        "\n",
        "    # final pareto\n",
        "    final_fronts = fast_non_dominated_sort(objs)\n",
        "    pareto_idx = final_fronts[0]\n",
        "\n",
        "    pareto_solutions = [(population[i], objs[i]) for i in pareto_idx]\n",
        "    return population, objs, pareto_solutions\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4) Run NSGA-II\n",
        "# ============================================================\n",
        "# Example usage:\n",
        "# population, objs, pareto = nsga2_optimize(\n",
        "#     X_train, y_train, X_val, y_val,\n",
        "#     pop_size=60,\n",
        "#     generations=20,\n",
        "#     eval_epochs=15,\n",
        "#     verbose=True,\n",
        "# )\n",
        "#\n",
        "# print(\"\\n=== Final Pareto Solutions (first 5) ===\")\n",
        "# for chrom, (rmse, n_params, n_feats) in pareto[:5]:\n",
        "#     print(f\"RMSE={rmse:.4f}, Params={int(n_params)}, Features={n_feats}, L={chrom.num_layers}, Act={chrom.activation}\")\n"
      ],
      "metadata": {
        "id": "3B5aS1rb1i5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "population, objs, pareto = nsga2_optimize(\n",
        "     X_train, y_train, X_val, y_val,\n",
        "     pop_size=60,\n",
        "     generations=20,\n",
        "     eval_epochs=15,\n",
        "     verbose=True,\n",
        " )\n",
        "#\n",
        "print(\"\\n=== Final Pareto Solutions (first 5) ===\")\n",
        "for chrom, (rmse, n_params, n_feats) in pareto[:5]:\n",
        "     print(f\"RMSE={rmse:.4f}, Params={int(n_params)}, Features={n_feats}, L={chrom.num_layers}, Act={chrom.activation}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PaHvJgY1k5A",
        "outputId": "5e9dd064-fff7-423b-fcbf-2747deaf0f84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gen 001 | Pareto size=33 | Best RMSE=13.9332 | Min Feats=8 | Min Params=239\n",
            "Gen 002 | Pareto size=42 | Best RMSE=13.6918 | Min Feats=8 | Min Params=239\n",
            "Gen 003 | Pareto size=55 | Best RMSE=13.5814 | Min Feats=8 | Min Params=239\n",
            "Gen 004 | Pareto size=60 | Best RMSE=13.1769 | Min Feats=8 | Min Params=239\n",
            "Gen 005 | Pareto size=60 | Best RMSE=13.1769 | Min Feats=8 | Min Params=239\n",
            "Gen 006 | Pareto size=60 | Best RMSE=13.1769 | Min Feats=8 | Min Params=239\n",
            "Gen 007 | Pareto size=60 | Best RMSE=13.1769 | Min Feats=8 | Min Params=239\n",
            "Gen 008 | Pareto size=60 | Best RMSE=13.1769 | Min Feats=8 | Min Params=239\n",
            "Gen 009 | Pareto size=60 | Best RMSE=13.1769 | Min Feats=8 | Min Params=239\n",
            "Gen 010 | Pareto size=60 | Best RMSE=13.1769 | Min Feats=8 | Min Params=239\n",
            "Gen 011 | Pareto size=60 | Best RMSE=13.0545 | Min Feats=8 | Min Params=222\n",
            "Gen 012 | Pareto size=60 | Best RMSE=12.7487 | Min Feats=8 | Min Params=222\n",
            "Gen 013 | Pareto size=60 | Best RMSE=12.7487 | Min Feats=8 | Min Params=222\n",
            "Gen 014 | Pareto size=60 | Best RMSE=12.7487 | Min Feats=8 | Min Params=222\n",
            "Gen 015 | Pareto size=60 | Best RMSE=12.7487 | Min Feats=8 | Min Params=222\n",
            "Gen 016 | Pareto size=60 | Best RMSE=12.7487 | Min Feats=8 | Min Params=222\n",
            "Gen 017 | Pareto size=60 | Best RMSE=12.7487 | Min Feats=8 | Min Params=205\n",
            "Gen 018 | Pareto size=60 | Best RMSE=12.7487 | Min Feats=8 | Min Params=205\n",
            "Gen 019 | Pareto size=60 | Best RMSE=12.7487 | Min Feats=8 | Min Params=205\n",
            "Gen 020 | Pareto size=60 | Best RMSE=12.7487 | Min Feats=8 | Min Params=171\n",
            "\n",
            "=== Final Pareto Solutions (first 5) ===\n",
            "RMSE=15.2870, Params=371073, Features=8, L=4, Act=relu\n",
            "RMSE=12.7487, Params=486187, Features=31, L=4, Act=relu\n",
            "RMSE=16.8781, Params=3241, Features=38, L=1, Act=leaky_relu\n",
            "RMSE=13.4417, Params=493388, Features=18, L=4, Act=relu\n",
            "RMSE=29.5555, Params=171, Features=8, L=1, Act=relu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Random Search"
      ],
      "metadata": {
        "id": "1_QwyiQNBqmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_search(\n",
        "    X_train, y_train, X_val, y_val,\n",
        "    n_trials=200,\n",
        "    eval_epochs=20,\n",
        "    seed=42,\n",
        "    verbose=True,\n",
        "):\n",
        "    np.random.seed(seed)\n",
        "    n_features = X_train.shape[1]\n",
        "\n",
        "    results = []  # (chrom, (rmse, params, feats))\n",
        "\n",
        "    for t in range(1, n_trials + 1):\n",
        "        chrom = random_chromosome(n_features=n_features)\n",
        "\n",
        "        obj = evaluate_chromosome(\n",
        "            chrom,\n",
        "            X_train, y_train,\n",
        "            X_val, y_val,\n",
        "            epochs=eval_epochs,\n",
        "            batch_size=256,\n",
        "            min_features=5,\n",
        "        )\n",
        "\n",
        "        results.append((chrom, obj))\n",
        "\n",
        "        if verbose and t % 20 == 0:\n",
        "            rmse, params, feats = obj\n",
        "            print(f\"[RandomSearch] Trial {t:04d}/{n_trials} | RMSE={rmse:.4f} | Params={int(params)} | Feats={feats}\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "xh9RrMAr7Pks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rs_results = random_search(\n",
        "    X_train, y_train, X_val, y_val,\n",
        "    n_trials=200,\n",
        "    eval_epochs=15\n",
        ")\n",
        "\n",
        "# Best by RMSE (single-objective view)\n",
        "best = min(rs_results, key=lambda x: x[1][0])\n",
        "print(\"Best RandomSearch RMSE:\", best[1][0], \"Features:\", best[1][2], \"Params:\", int(best[1][1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYsqPV_eBLFu",
        "outputId": "67d3d5fc-34e3-4309-9fcd-3ee25c40be38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RandomSearch] Trial 0020/200 | RMSE=21.8696 | Params=239 | Feats=12\n",
            "[RandomSearch] Trial 0040/200 | RMSE=37.2296 | Params=1081 | Feats=28\n",
            "[RandomSearch] Trial 0060/200 | RMSE=15.6968 | Params=41690 | Feats=33\n",
            "[RandomSearch] Trial 0080/200 | RMSE=15.2213 | Params=135464 | Feats=25\n",
            "[RandomSearch] Trial 0100/200 | RMSE=14.9517 | Params=242958 | Feats=23\n",
            "[RandomSearch] Trial 0120/200 | RMSE=17.0529 | Params=91254 | Feats=10\n",
            "[RandomSearch] Trial 0140/200 | RMSE=18.9131 | Params=13295 | Feats=32\n",
            "[RandomSearch] Trial 0160/200 | RMSE=21.4045 | Params=273863 | Feats=20\n",
            "[RandomSearch] Trial 0180/200 | RMSE=40.9771 | Params=29439 | Feats=37\n",
            "[RandomSearch] Trial 0200/200 | RMSE=31.8832 | Params=2738 | Feats=21\n",
            "Best RandomSearch RMSE: 13.801971980191533 Features: 26 Params: 234034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Grid Search"
      ],
      "metadata": {
        "id": "i3XdaQXUBuJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def make_chromosome(\n",
        "    n_total_features: int,\n",
        "    k_features: int,\n",
        "    num_layers: int,\n",
        "    hidden_units: list,\n",
        "    activation: str,\n",
        "    dropout: float = 0.1,\n",
        "    lr: float = 1e-3,\n",
        "    weight_decay: float = 1e-5,\n",
        ") -> Chromosome:\n",
        "    \"\"\"\n",
        "    hidden_units must be length 4 (we only use first num_layers)\n",
        "    \"\"\"\n",
        "    # Randomly pick k features (wrapper-style)\n",
        "    mask = np.zeros(n_total_features, dtype=np.int32)\n",
        "    idx = np.random.choice(n_total_features, size=k_features, replace=False)\n",
        "    mask[idx] = 1\n",
        "\n",
        "    hu = np.array(hidden_units, dtype=np.int32)\n",
        "    assert hu.shape[0] == 4, \"hidden_units must have length 4\"\n",
        "\n",
        "    return Chromosome(\n",
        "        feature_mask=mask,\n",
        "        num_layers=int(num_layers),\n",
        "        hidden_units=hu,\n",
        "        activation=str(activation),\n",
        "        dropout=float(dropout),\n",
        "        lr=float(lr),\n",
        "        weight_decay=float(weight_decay),\n",
        "    )\n"
      ],
      "metadata": {
        "id": "lzuDKlVrx_6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "def grid_search(\n",
        "    X_train, y_train, X_val, y_val,\n",
        "    k_feature_grid=(8, 12, 20, 30, 40, 60, 81),\n",
        "    layer_grid=(1, 2, 3),\n",
        "    width_grid=((64,), (128,), (256,), (256,128), (256,128,64)),\n",
        "    activation_grid=(\"relu\", \"gelu\"),\n",
        "    repeats_per_setting=2,        # repeat each grid point with different random feature subsets\n",
        "    eval_epochs=20,\n",
        "    seed=42,\n",
        "    verbose=True,\n",
        "):\n",
        "    np.random.seed(seed)\n",
        "    n_total_features = X_train.shape[1]\n",
        "\n",
        "    results = []  # (chrom, (rmse, params, feats))\n",
        "\n",
        "    # Convert widths into 4-length hidden_units arrays\n",
        "    def pad_widths(widths):\n",
        "        padded = list(widths) + [16] * (4 - len(widths))  # fillers won't be used if num_layers < len(widths)\n",
        "        return padded[:4]\n",
        "\n",
        "    grid = list(itertools.product(k_feature_grid, layer_grid, width_grid, activation_grid))\n",
        "\n",
        "    total_trials = len(grid) * repeats_per_setting\n",
        "    trial = 0\n",
        "\n",
        "    for (k, L, widths, act) in grid:\n",
        "        # skip invalid combos (can't have L > len(widths))\n",
        "        if L > len(widths):\n",
        "            continue\n",
        "\n",
        "        hidden_4 = pad_widths(widths)\n",
        "\n",
        "        for _ in range(repeats_per_setting):\n",
        "            trial += 1\n",
        "\n",
        "            chrom = make_chromosome(\n",
        "                n_total_features=n_total_features,\n",
        "                k_features=int(k),\n",
        "                num_layers=int(L),\n",
        "                hidden_units=hidden_4,\n",
        "                activation=act,\n",
        "                dropout=0.1,\n",
        "                lr=1e-3,\n",
        "                weight_decay=1e-5\n",
        "            )\n",
        "\n",
        "            obj = evaluate_chromosome(\n",
        "                chrom,\n",
        "                X_train, y_train,\n",
        "                X_val, y_val,\n",
        "                epochs=eval_epochs,\n",
        "                batch_size=256,\n",
        "                min_features=5,\n",
        "            )\n",
        "\n",
        "            results.append((chrom, obj))\n",
        "\n",
        "            if verbose and trial % 20 == 0:\n",
        "                rmse, params, feats = obj\n",
        "                print(f\"[GridSearch] Trial {trial:04d}/{total_trials} | RMSE={rmse:.4f} | Params={int(params)} | Feats={feats}\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "YIKIn1MXxO3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gs_results = grid_search(\n",
        "    X_train, y_train, X_val, y_val,\n",
        "    k_feature_grid=(8, 12, 20, 40, 81),\n",
        "    layer_grid=(1, 2, 3),\n",
        "    width_grid=((64,), (128,), (256,), (256,128), (256,128,64)),\n",
        "    activation_grid=(\"relu\", \"gelu\"),\n",
        "    repeats_per_setting=2,\n",
        "    eval_epochs=15\n",
        ")\n",
        "\n",
        "best_gs = min(gs_results, key=lambda x: x[1][0])\n",
        "print(\"Best GridSearch RMSE:\", best_gs[1][0], \"Features:\", best_gs[1][2], \"Params:\", int(best_gs[1][1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0Gw9l5FxYMy",
        "outputId": "bab4340b-0ba8-4050-997d-da00882dc5fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GridSearch] Trial 0020/300 | RMSE=21.3228 | Params=2561 | Feats=8\n",
            "[GridSearch] Trial 0040/300 | RMSE=20.7464 | Params=1793 | Feats=12\n",
            "[GridSearch] Trial 0060/300 | RMSE=17.9287 | Params=36353 | Feats=12\n",
            "[GridSearch] Trial 0080/300 | RMSE=18.2716 | Params=5633 | Feats=20\n",
            "[GridSearch] Trial 0100/300 | RMSE=17.1573 | Params=2689 | Feats=40\n",
            "[GridSearch] Trial 0120/300 | RMSE=15.2492 | Params=43521 | Feats=40\n",
            "[GridSearch] Trial 0140/300 | RMSE=15.3428 | Params=21249 | Feats=81\n",
            "[GridSearch] Trial 0160/300 | RMSE=13.9287 | Params=62209 | Feats=81\n",
            "Best GridSearch RMSE: 13.92567003374243 Features: 81 Params: 62209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "H4KXAUbIwKMd"
      }
    }
  ]
}