{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "heHWTGtMwBDu",
        "outputId": "0d551612-c097-445a-8952-0c84e803c032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2770072040.py:12: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.1).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/munumbutt/superconductor-dataset?dataset_version_number=1&file_name=train.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7.57M/7.57M [00:00<00:00, 10.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting zip of train.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 records:    number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass  \\\n",
            "0                   4         88.944468             57.862692   \n",
            "1                   5         92.729214             58.518416   \n",
            "2                   4         88.944468             57.885242   \n",
            "3                   4         88.944468             57.873967   \n",
            "4                   4         88.944468             57.840143   \n",
            "\n",
            "   gmean_atomic_mass  wtd_gmean_atomic_mass  entropy_atomic_mass  \\\n",
            "0          66.361592              36.116612             1.181795   \n",
            "1          73.132787              36.396602             1.449309   \n",
            "2          66.361592              36.122509             1.181795   \n",
            "3          66.361592              36.119560             1.181795   \n",
            "4          66.361592              36.110716             1.181795   \n",
            "\n",
            "   wtd_entropy_atomic_mass  range_atomic_mass  wtd_range_atomic_mass  \\\n",
            "0                 1.062396          122.90607              31.794921   \n",
            "1                 1.057755          122.90607              36.161939   \n",
            "2                 0.975980          122.90607              35.741099   \n",
            "3                 1.022291          122.90607              33.768010   \n",
            "4                 1.129224          122.90607              27.848743   \n",
            "\n",
            "   std_atomic_mass  ...  wtd_mean_Valence  gmean_Valence  wtd_gmean_Valence  \\\n",
            "0        51.968828  ...          2.257143       2.213364           2.219783   \n",
            "1        47.094633  ...          2.257143       1.888175           2.210679   \n",
            "2        51.968828  ...          2.271429       2.213364           2.232679   \n",
            "3        51.968828  ...          2.264286       2.213364           2.226222   \n",
            "4        51.968828  ...          2.242857       2.213364           2.206963   \n",
            "\n",
            "   entropy_Valence  wtd_entropy_Valence  range_Valence  wtd_range_Valence  \\\n",
            "0         1.368922             1.066221              1           1.085714   \n",
            "1         1.557113             1.047221              2           1.128571   \n",
            "2         1.368922             1.029175              1           1.114286   \n",
            "3         1.368922             1.048834              1           1.100000   \n",
            "4         1.368922             1.096052              1           1.057143   \n",
            "\n",
            "   std_Valence  wtd_std_Valence  critical_temp  \n",
            "0     0.433013         0.437059           29.0  \n",
            "1     0.632456         0.468606           26.0  \n",
            "2     0.433013         0.444697           19.0  \n",
            "3     0.433013         0.440952           22.0  \n",
            "4     0.433013         0.428809           23.0  \n",
            "\n",
            "[5 rows x 82 columns]\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies as needed:\n",
        "# pip install kagglehub[pandas-datasets]\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# Set the path to the file you'd like to load\n",
        "# You need to specify a file name from the dataset, e.g., 'superconductor_data.csv'\n",
        "# You can check the dataset page on Kaggle for available files.\n",
        "file_path = \"train.csv\"\n",
        "\n",
        "# Load the latest version\n",
        "df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"munumbutt/superconductor-dataset\",\n",
        "  file_path,\n",
        "  # Provide any additional arguments like\n",
        "  # sql_query or pandas_kwargs. See the\n",
        "  # documenation for more information:\n",
        "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
        ")\n",
        "\n",
        "print(\"First 5 records:\", df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "l1VWRjiRxZkb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 2) Split features / label\n",
        "# =========================\n",
        "# Most common label column name for this dataset is \"critical_temp\"\n",
        "# If your label column differs, update it here.\n",
        "target_col = \"critical_temp\"\n",
        "assert target_col in df.columns, f\"Target column '{target_col}' not found. Columns: {df.columns.tolist()}\"\n",
        "\n",
        "X = df.drop(columns=[target_col]).values.astype(np.float32)\n",
        "y = df[target_col].values.astype(np.float32).reshape(-1, 1)\n",
        "\n",
        "print(\"X shape:\", X.shape, \"| y shape:\", y.shape)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 3) Train / Val / Test split\n",
        "# =========================\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 4) Standardize (fit on train only!)\n",
        "# =========================\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
        "X_val = scaler.transform(X_val).astype(np.float32)\n",
        "X_test = scaler.transform(X_test).astype(np.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJsPb5JkBYhe",
        "outputId": "b748cd04-6a6d-4b7f-dc67-9978e23bd566"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (21263, 81) | y shape: (21263, 1)\n",
            "Train: (17010, 81) Val: (2126, 81) Test: (2127, 81)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 5) Build PyTorch DataLoaders\n",
        "# =========================\n",
        "batch_size = 256\n",
        "\n",
        "train_ds = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
        "val_ds = TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n",
        "test_ds = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 6) Define a baseline 3-layer MLP\n",
        "# =========================\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dims=(256, 128, 64), dropout=0.1):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev = input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers.append(nn.Linear(prev, h))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            prev = h\n",
        "        layers.append(nn.Linear(prev, 1))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model = MLP(input_dim=X_train.shape[1], hidden_dims=(256, 128, 64), dropout=0.1).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 7) Train loop + early stopping\n",
        "# =========================\n",
        "def evaluate_rmse(model, loader):\n",
        "    model.eval()\n",
        "    preds, trues = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            out = model(xb)\n",
        "            preds.append(out.cpu().numpy())\n",
        "            trues.append(yb.cpu().numpy())\n",
        "    preds = np.vstack(preds)\n",
        "    trues = np.vstack(trues)\n",
        "    rmse = np.sqrt(mean_squared_error(trues, preds))\n",
        "    return rmse\n",
        "\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "best_state = None\n",
        "patience = 10\n",
        "pat_counter = 0\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(xb)\n",
        "        loss = criterion(pred, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "\n",
        "    train_loss = total_loss / len(train_loader.dataset)\n",
        "    val_rmse = evaluate_rmse(model, val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch:03d} | TrainLoss={train_loss:.4f} | ValRMSE={val_rmse:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_rmse < best_val:\n",
        "        best_val = val_rmse\n",
        "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "        pat_counter = 0\n",
        "    else:\n",
        "        pat_counter += 1\n",
        "        if pat_counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Load best checkpoint\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "\n",
        "test_rmse = evaluate_rmse(model, test_loader)\n",
        "print(\"\\n✅ Final Results\")\n",
        "print(f\"Best Val RMSE: {best_val:.4f}\")\n",
        "print(f\"Test RMSE:     {test_rmse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "l8-FLsTny71x",
        "outputId": "8eee1daf-90f8-40f7-c74c-df1ec33fcc52"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (21263, 81) | y shape: (21263, 1)\n",
            "Train: (17010, 81) Val: (2126, 81) Test: (2127, 81)\n",
            "Using device: cpu\n",
            "Epoch 001 | TrainLoss=1095.2989 | ValRMSE=19.0967\n",
            "Epoch 002 | TrainLoss=357.4573 | ValRMSE=17.5802\n",
            "Epoch 003 | TrainLoss=318.3944 | ValRMSE=16.9247\n",
            "Epoch 004 | TrainLoss=296.4261 | ValRMSE=16.2026\n",
            "Epoch 005 | TrainLoss=277.5923 | ValRMSE=15.7907\n",
            "Epoch 006 | TrainLoss=263.8711 | ValRMSE=15.4300\n",
            "Epoch 007 | TrainLoss=258.1943 | ValRMSE=15.2539\n",
            "Epoch 008 | TrainLoss=249.1535 | ValRMSE=15.0343\n",
            "Epoch 009 | TrainLoss=246.5435 | ValRMSE=14.9137\n",
            "Epoch 010 | TrainLoss=234.9795 | ValRMSE=14.8450\n",
            "Epoch 011 | TrainLoss=230.5637 | ValRMSE=14.4098\n",
            "Epoch 012 | TrainLoss=225.1633 | ValRMSE=14.4064\n",
            "Epoch 013 | TrainLoss=222.2740 | ValRMSE=14.2428\n",
            "Epoch 014 | TrainLoss=217.7345 | ValRMSE=14.6198\n",
            "Epoch 015 | TrainLoss=215.9252 | ValRMSE=14.0366\n",
            "Epoch 016 | TrainLoss=211.5936 | ValRMSE=13.8652\n",
            "Epoch 017 | TrainLoss=207.6658 | ValRMSE=13.7064\n",
            "Epoch 018 | TrainLoss=207.2734 | ValRMSE=13.5953\n",
            "Epoch 019 | TrainLoss=204.8396 | ValRMSE=13.4706\n",
            "Epoch 020 | TrainLoss=203.6056 | ValRMSE=13.9335\n",
            "Epoch 021 | TrainLoss=200.6592 | ValRMSE=13.4771\n",
            "Epoch 022 | TrainLoss=193.7194 | ValRMSE=13.5566\n",
            "Epoch 023 | TrainLoss=193.1814 | ValRMSE=13.6703\n",
            "Epoch 024 | TrainLoss=194.8970 | ValRMSE=13.1230\n",
            "Epoch 025 | TrainLoss=192.2181 | ValRMSE=14.0176\n",
            "Epoch 026 | TrainLoss=192.0575 | ValRMSE=13.0711\n",
            "Epoch 027 | TrainLoss=188.2615 | ValRMSE=12.9503\n",
            "Epoch 028 | TrainLoss=188.5532 | ValRMSE=13.2270\n",
            "Epoch 029 | TrainLoss=186.9711 | ValRMSE=13.2764\n",
            "Epoch 030 | TrainLoss=186.2900 | ValRMSE=12.9153\n",
            "Epoch 031 | TrainLoss=185.2546 | ValRMSE=12.8329\n",
            "Epoch 032 | TrainLoss=183.3900 | ValRMSE=12.6755\n",
            "Epoch 033 | TrainLoss=183.3288 | ValRMSE=12.8881\n",
            "Epoch 034 | TrainLoss=179.8349 | ValRMSE=12.7207\n",
            "Epoch 035 | TrainLoss=175.8069 | ValRMSE=12.7735\n",
            "Epoch 036 | TrainLoss=173.1126 | ValRMSE=12.5488\n",
            "Epoch 037 | TrainLoss=173.2115 | ValRMSE=12.4846\n",
            "Epoch 038 | TrainLoss=174.4801 | ValRMSE=12.3866\n",
            "Epoch 039 | TrainLoss=171.1339 | ValRMSE=12.3940\n",
            "Epoch 040 | TrainLoss=169.7250 | ValRMSE=12.3669\n",
            "Epoch 041 | TrainLoss=168.7535 | ValRMSE=12.4215\n",
            "Epoch 042 | TrainLoss=171.5859 | ValRMSE=12.9070\n",
            "Epoch 043 | TrainLoss=168.8605 | ValRMSE=12.1959\n",
            "Epoch 044 | TrainLoss=171.6128 | ValRMSE=12.6341\n",
            "Epoch 045 | TrainLoss=166.0445 | ValRMSE=12.4040\n",
            "Epoch 046 | TrainLoss=165.8119 | ValRMSE=12.0478\n",
            "Epoch 047 | TrainLoss=165.6590 | ValRMSE=12.4064\n",
            "Epoch 048 | TrainLoss=164.2526 | ValRMSE=12.4459\n",
            "Epoch 049 | TrainLoss=165.0218 | ValRMSE=12.1577\n",
            "Epoch 050 | TrainLoss=162.9811 | ValRMSE=12.0574\n",
            "Epoch 051 | TrainLoss=160.5462 | ValRMSE=11.9642\n",
            "Epoch 052 | TrainLoss=157.7022 | ValRMSE=12.4660\n",
            "Epoch 053 | TrainLoss=160.2008 | ValRMSE=11.9591\n",
            "Epoch 054 | TrainLoss=159.2809 | ValRMSE=11.9470\n",
            "Epoch 055 | TrainLoss=156.1183 | ValRMSE=12.0799\n",
            "Epoch 056 | TrainLoss=154.9183 | ValRMSE=12.0385\n",
            "Epoch 057 | TrainLoss=154.7905 | ValRMSE=11.9194\n",
            "Epoch 058 | TrainLoss=153.2207 | ValRMSE=11.9532\n",
            "Epoch 059 | TrainLoss=157.4764 | ValRMSE=11.8414\n",
            "Epoch 060 | TrainLoss=157.8946 | ValRMSE=11.8481\n",
            "Epoch 061 | TrainLoss=155.8534 | ValRMSE=11.8010\n",
            "Epoch 062 | TrainLoss=150.9988 | ValRMSE=11.6573\n",
            "Epoch 063 | TrainLoss=149.2839 | ValRMSE=11.6256\n",
            "Epoch 064 | TrainLoss=151.5564 | ValRMSE=11.8713\n",
            "Epoch 065 | TrainLoss=153.1264 | ValRMSE=11.7257\n",
            "Epoch 066 | TrainLoss=151.0763 | ValRMSE=11.5647\n",
            "Epoch 067 | TrainLoss=147.3390 | ValRMSE=11.5870\n",
            "Epoch 068 | TrainLoss=147.4937 | ValRMSE=11.5918\n",
            "Epoch 069 | TrainLoss=145.4205 | ValRMSE=11.4878\n",
            "Epoch 070 | TrainLoss=147.9722 | ValRMSE=11.4545\n",
            "Epoch 071 | TrainLoss=144.3549 | ValRMSE=11.3863\n",
            "Epoch 072 | TrainLoss=144.6717 | ValRMSE=11.4408\n",
            "Epoch 073 | TrainLoss=149.4934 | ValRMSE=11.4324\n",
            "Epoch 074 | TrainLoss=143.1767 | ValRMSE=11.2976\n",
            "Epoch 075 | TrainLoss=141.5636 | ValRMSE=11.2026\n",
            "Epoch 076 | TrainLoss=140.9561 | ValRMSE=11.1826\n",
            "Epoch 077 | TrainLoss=141.6771 | ValRMSE=11.2852\n",
            "Epoch 078 | TrainLoss=139.0705 | ValRMSE=11.2223\n",
            "Epoch 079 | TrainLoss=138.9351 | ValRMSE=11.0187\n",
            "Epoch 080 | TrainLoss=140.3874 | ValRMSE=11.2611\n",
            "Epoch 081 | TrainLoss=138.8577 | ValRMSE=11.0968\n",
            "Epoch 082 | TrainLoss=139.5560 | ValRMSE=11.2959\n",
            "Epoch 083 | TrainLoss=138.2500 | ValRMSE=11.2794\n",
            "Epoch 084 | TrainLoss=136.7735 | ValRMSE=11.3237\n",
            "Epoch 085 | TrainLoss=134.6082 | ValRMSE=12.3654\n",
            "Epoch 086 | TrainLoss=138.0543 | ValRMSE=10.9827\n",
            "Epoch 087 | TrainLoss=141.5189 | ValRMSE=11.3268\n",
            "Epoch 088 | TrainLoss=135.8742 | ValRMSE=11.2596\n",
            "Epoch 089 | TrainLoss=136.2713 | ValRMSE=10.9550\n",
            "Epoch 090 | TrainLoss=140.3615 | ValRMSE=11.1243\n",
            "Epoch 091 | TrainLoss=135.7529 | ValRMSE=10.8659\n",
            "Epoch 092 | TrainLoss=135.7882 | ValRMSE=10.9563\n",
            "Epoch 093 | TrainLoss=130.9790 | ValRMSE=11.1162\n",
            "Epoch 094 | TrainLoss=136.7798 | ValRMSE=10.8501\n",
            "Epoch 095 | TrainLoss=134.4189 | ValRMSE=10.8449\n",
            "Epoch 096 | TrainLoss=131.8378 | ValRMSE=11.0097\n",
            "Epoch 097 | TrainLoss=133.9229 | ValRMSE=11.0618\n",
            "Epoch 098 | TrainLoss=131.3859 | ValRMSE=10.7653\n",
            "Epoch 099 | TrainLoss=130.4265 | ValRMSE=11.0322\n",
            "Epoch 100 | TrainLoss=133.3983 | ValRMSE=10.9377\n",
            "\n",
            "✅ Final Results\n",
            "Best Val RMSE: 10.7653\n",
            "Test RMSE:     11.0110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from dataclasses import dataclass\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 1) Chromosome Definition\n",
        "# =========================\n",
        "ACTIVATIONS = [\"relu\", \"gelu\", \"tanh\", \"leaky_relu\"]\n",
        "\n",
        "@dataclass\n",
        "class Chromosome:\n",
        "    # Feature selection\n",
        "    feature_mask: np.ndarray  # shape (81,), dtype=bool or int {0,1}\n",
        "\n",
        "    # Architecture\n",
        "    num_layers: int           # 1..4\n",
        "    hidden_units: np.ndarray  # shape (4,), only first num_layers used\n",
        "    activation: str           # in ACTIVATIONS\n",
        "\n",
        "    # Optional (nice for performance)\n",
        "    dropout: float = 0.1\n",
        "    lr: float = 1e-3\n",
        "    weight_decay: float = 1e-5\n",
        "\n",
        "\n",
        "def random_chromosome(n_features=81) -> Chromosome:\n",
        "    # feature mask: keep ~10-40 features initially (prevents degenerate too-sparse)\n",
        "    mask = np.zeros(n_features, dtype=np.int32)\n",
        "    k = np.random.randint(8, 41)\n",
        "    idx = np.random.choice(n_features, size=k, replace=False)\n",
        "    mask[idx] = 1\n",
        "\n",
        "    num_layers = np.random.randint(1, 5)  # 1..4\n",
        "    hidden_units = np.random.randint(16, 513, size=(4,))  # 16..512\n",
        "\n",
        "    activation = np.random.choice(ACTIVATIONS)\n",
        "\n",
        "    dropout = np.random.uniform(0.0, 0.4)\n",
        "    lr = 10 ** np.random.uniform(-4, -3)  # 1e-4 to 1e-3\n",
        "    wd = np.random.uniform(0.0, 1e-3)\n",
        "\n",
        "    return Chromosome(\n",
        "        feature_mask=mask,\n",
        "        num_layers=num_layers,\n",
        "        hidden_units=hidden_units,\n",
        "        activation=activation,\n",
        "        dropout=float(dropout),\n",
        "        lr=float(lr),\n",
        "        weight_decay=float(wd),\n",
        "    )\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 2) Decode -> Build PyTorch model\n",
        "# =========================\n",
        "def get_activation(name: str):\n",
        "    if name == \"relu\":\n",
        "        return nn.ReLU()\n",
        "    if name == \"gelu\":\n",
        "        return nn.GELU()\n",
        "    if name == \"tanh\":\n",
        "        return nn.Tanh()\n",
        "    if name == \"leaky_relu\":\n",
        "        return nn.LeakyReLU(0.1)\n",
        "    raise ValueError(f\"Unknown activation: {name}\")\n",
        "\n",
        "\n",
        "class EvoMLP(nn.Module):\n",
        "    def __init__(self, input_dim, num_layers, hidden_units, activation, dropout):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev = input_dim\n",
        "        act = get_activation(activation)\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            h = int(hidden_units[i])\n",
        "            layers.append(nn.Linear(prev, h))\n",
        "            layers.append(act)\n",
        "            if dropout > 0:\n",
        "                layers.append(nn.Dropout(dropout))\n",
        "            prev = h\n",
        "\n",
        "        layers.append(nn.Linear(prev, 1))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def count_params(model: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 3) Evaluate Chromosome -> 3 objectives\n",
        "#    f1: RMSE (val)\n",
        "#    f2: #params\n",
        "#    f3: #selected features\n",
        "# =========================\n",
        "def evaluate_chromosome(\n",
        "    chrom: Chromosome,\n",
        "    X_train, y_train, X_val, y_val,\n",
        "    epochs=30,\n",
        "    batch_size=256,\n",
        "    device=None,\n",
        "    min_features=5\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns: (rmse, n_params, n_features_selected)\n",
        "    All minimized.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # --- Feature mask ---\n",
        "    mask = chrom.feature_mask.astype(bool)\n",
        "    n_selected = int(mask.sum())\n",
        "\n",
        "    # Constraint: avoid too few features\n",
        "    if n_selected < min_features:\n",
        "        # Huge penalty so NSGA-II will avoid it\n",
        "        return (1e9, 1e9, n_selected)\n",
        "\n",
        "    Xtr = X_train[:, mask]\n",
        "    Xva = X_val[:, mask]\n",
        "\n",
        "    # Torch tensors\n",
        "    Xtr_t = torch.tensor(Xtr, dtype=torch.float32)\n",
        "    ytr_t = torch.tensor(y_train, dtype=torch.float32)\n",
        "    Xva_t = torch.tensor(Xva, dtype=torch.float32)\n",
        "    yva_t = torch.tensor(y_val, dtype=torch.float32)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.TensorDataset(Xtr_t, ytr_t),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    model = EvoMLP(\n",
        "        input_dim=Xtr.shape[1],\n",
        "        num_layers=chrom.num_layers,\n",
        "        hidden_units=chrom.hidden_units,\n",
        "        activation=chrom.activation,\n",
        "        dropout=chrom.dropout,\n",
        "    ).to(device)\n",
        "\n",
        "    n_params = count_params(model)\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=chrom.lr,\n",
        "        weight_decay=chrom.weight_decay,\n",
        "    )\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    # --- training (fixed small budget) ---\n",
        "    model.train()\n",
        "    for _ in range(epochs):\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(xb)\n",
        "            loss = loss_fn(pred, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # --- validation RMSE ---\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = model(Xva_t.to(device)).cpu().numpy().reshape(-1)\n",
        "        trues = yva_t.cpu().numpy().reshape(-1)\n",
        "        rmse = float(np.sqrt(mean_squared_error(trues, preds)))\n",
        "\n",
        "    return (rmse, n_params, n_selected)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 4) Genetic Operators\n",
        "# =========================\n",
        "def uniform_crossover(mask1, mask2, p=0.5):\n",
        "    \"\"\"Bitwise uniform crossover\"\"\"\n",
        "    assert mask1.shape == mask2.shape\n",
        "    swap = np.random.rand(mask1.shape[0]) < p\n",
        "    child1 = mask1.copy()\n",
        "    child2 = mask2.copy()\n",
        "    child1[swap], child2[swap] = child2[swap], child1[swap]\n",
        "    return child1, child2\n",
        "\n",
        "\n",
        "def crossover(parent1: Chromosome, parent2: Chromosome) -> tuple[Chromosome, Chromosome]:\n",
        "    # Feature mask crossover\n",
        "    m1, m2 = uniform_crossover(parent1.feature_mask, parent2.feature_mask, p=0.5)\n",
        "\n",
        "    # num_layers crossover (pick one)\n",
        "    L1 = parent1.num_layers if np.random.rand() < 0.5 else parent2.num_layers\n",
        "    L2 = parent2.num_layers if np.random.rand() < 0.5 else parent1.num_layers\n",
        "\n",
        "    # hidden units crossover (per-gene)\n",
        "    hu1 = parent1.hidden_units.copy()\n",
        "    hu2 = parent2.hidden_units.copy()\n",
        "    for i in range(4):\n",
        "        if np.random.rand() < 0.5:\n",
        "            hu1[i], hu2[i] = hu2[i], hu1[i]\n",
        "\n",
        "    # activation crossover\n",
        "    a1 = parent1.activation if np.random.rand() < 0.5 else parent2.activation\n",
        "    a2 = parent2.activation if np.random.rand() < 0.5 else parent1.activation\n",
        "\n",
        "    # dropout/lr/wd crossover (simple averaging)\n",
        "    d1 = float((parent1.dropout + parent2.dropout) / 2.0)\n",
        "    d2 = d1\n",
        "    lr1 = float((parent1.lr + parent2.lr) / 2.0)\n",
        "    lr2 = lr1\n",
        "    wd1 = float((parent1.weight_decay + parent2.weight_decay) / 2.0)\n",
        "    wd2 = wd1\n",
        "\n",
        "    return (\n",
        "        Chromosome(m1, L1, hu1, a1, d1, lr1, wd1),\n",
        "        Chromosome(m2, L2, hu2, a2, d2, lr2, wd2),\n",
        "    )\n",
        "\n",
        "\n",
        "def mutate(chrom: Chromosome, p_mask=0.03, p_arch=0.2, n_features=81) -> Chromosome:\n",
        "    c = Chromosome(\n",
        "        feature_mask=chrom.feature_mask.copy(),\n",
        "        num_layers=int(chrom.num_layers),\n",
        "        hidden_units=chrom.hidden_units.copy(),\n",
        "        activation=str(chrom.activation),\n",
        "        dropout=float(chrom.dropout),\n",
        "        lr=float(chrom.lr),\n",
        "        weight_decay=float(chrom.weight_decay),\n",
        "    )\n",
        "\n",
        "    # --- Feature mask mutation (bit flip) ---\n",
        "    # flip each bit with small prob\n",
        "    flip = np.random.rand(n_features) < (p_mask / n_features * 81)  # scaled\n",
        "    c.feature_mask[flip] = 1 - c.feature_mask[flip]\n",
        "\n",
        "    # keep at least 1 feature alive (hard safety)\n",
        "    if c.feature_mask.sum() == 0:\n",
        "        c.feature_mask[np.random.randint(0, n_features)] = 1\n",
        "\n",
        "    # --- Architecture mutation ---\n",
        "    if np.random.rand() < p_arch:\n",
        "        # mutate num_layers\n",
        "        if np.random.rand() < 0.5:\n",
        "            c.num_layers = int(np.clip(c.num_layers + np.random.choice([-1, 1]), 1, 4))\n",
        "\n",
        "        # mutate hidden units (random reset per layer)\n",
        "        for i in range(4):\n",
        "            if np.random.rand() < 0.3:\n",
        "                c.hidden_units[i] = np.random.randint(16, 513)\n",
        "\n",
        "        # mutate activation\n",
        "        if np.random.rand() < 0.2:\n",
        "            c.activation = np.random.choice(ACTIVATIONS)\n",
        "\n",
        "        # mutate dropout / lr / wd\n",
        "        if np.random.rand() < 0.3:\n",
        "            c.dropout = float(np.clip(c.dropout + np.random.normal(0, 0.05), 0.0, 0.5))\n",
        "        if np.random.rand() < 0.3:\n",
        "            c.lr = float(np.clip(c.lr * (10 ** np.random.normal(0, 0.15)), 1e-4, 3e-3))\n",
        "        if np.random.rand() < 0.3:\n",
        "            c.weight_decay = float(np.clip(c.weight_decay + np.random.normal(0, 2e-4), 0.0, 1e-3))\n",
        "\n",
        "    return c\n"
      ],
      "metadata": {
        "id": "sGc-nOb7zuv3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: generate 1 chromosome, evaluate objectives\n",
        "chrom = random_chromosome(n_features=X_train.shape[1])\n",
        "\n",
        "rmse, n_params, n_feats = evaluate_chromosome(\n",
        "    chrom,\n",
        "    X_train, y_train,\n",
        "    X_val, y_val,\n",
        "    epochs=20\n",
        ")\n",
        "\n",
        "print(\"Objectives:\")\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"#Params:\", n_params)\n",
        "print(\"#Selected features:\", n_feats)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XgeuWV90GH5",
        "outputId": "ede1814d-1c2f-4514-dd19-61ff689e822a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Objectives:\n",
            "RMSE: 17.552136170018148\n",
            "#Params: 228385\n",
            "#Selected features: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "# -------------------------\n",
        "# You already have these from earlier:\n",
        "# - Chromosome\n",
        "# - random_chromosome()\n",
        "# - crossover()\n",
        "# - mutate()\n",
        "# - evaluate_chromosome()\n",
        "# -------------------------\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1) NSGA-II Core Utilities\n",
        "# ============================================================\n",
        "def dominates(obj_a: Tuple[float, float, float], obj_b: Tuple[float, float, float]) -> bool:\n",
        "    \"\"\"\n",
        "    True if A dominates B (all <= and at least one <), for minimization.\n",
        "    \"\"\"\n",
        "    return (obj_a[0] <= obj_b[0] and obj_a[1] <= obj_b[1] and obj_a[2] <= obj_b[2]) and \\\n",
        "           (obj_a[0] <  obj_b[0] or  obj_a[1] <  obj_b[1] or  obj_a[2] <  obj_b[2])\n",
        "\n",
        "\n",
        "def fast_non_dominated_sort(objs: List[Tuple[float, float, float]]) -> List[List[int]]:\n",
        "    \"\"\"\n",
        "    Returns fronts as list of lists of indices.\n",
        "    NSGA-II fast non-dominated sorting.\n",
        "    \"\"\"\n",
        "    N = len(objs)\n",
        "    S = [[] for _ in range(N)]   # who i dominates\n",
        "    n = [0] * N                  # domination count\n",
        "    fronts = [[]]\n",
        "\n",
        "    for p in range(N):\n",
        "        for q in range(N):\n",
        "            if p == q:\n",
        "                continue\n",
        "            if dominates(objs[p], objs[q]):\n",
        "                S[p].append(q)\n",
        "            elif dominates(objs[q], objs[p]):\n",
        "                n[p] += 1\n",
        "\n",
        "        if n[p] == 0:\n",
        "            fronts[0].append(p)\n",
        "\n",
        "    i = 0\n",
        "    while len(fronts[i]) > 0:\n",
        "        next_front = []\n",
        "        for p in fronts[i]:\n",
        "            for q in S[p]:\n",
        "                n[q] -= 1\n",
        "                if n[q] == 0:\n",
        "                    next_front.append(q)\n",
        "        i += 1\n",
        "        fronts.append(next_front)\n",
        "\n",
        "    fronts.pop()  # last one empty\n",
        "    return fronts\n",
        "\n",
        "\n",
        "def crowding_distance(front: List[int], objs: List[Tuple[float, float, float]]) -> Dict[int, float]:\n",
        "    \"\"\"\n",
        "    Compute crowding distance for a front. Higher is better.\n",
        "    \"\"\"\n",
        "    dist = {idx: 0.0 for idx in front}\n",
        "    if len(front) <= 2:\n",
        "        for idx in front:\n",
        "            dist[idx] = float(\"inf\")\n",
        "        return dist\n",
        "\n",
        "    M = 3  # number of objectives\n",
        "    for m in range(M):\n",
        "        front_sorted = sorted(front, key=lambda i: objs[i][m])\n",
        "        dist[front_sorted[0]] = float(\"inf\")\n",
        "        dist[front_sorted[-1]] = float(\"inf\")\n",
        "\n",
        "        f_min = objs[front_sorted[0]][m]\n",
        "        f_max = objs[front_sorted[-1]][m]\n",
        "        if f_max == f_min:\n",
        "            continue\n",
        "\n",
        "        for k in range(1, len(front_sorted) - 1):\n",
        "            prev_i = front_sorted[k - 1]\n",
        "            next_i = front_sorted[k + 1]\n",
        "            dist[front_sorted[k]] += (objs[next_i][m] - objs[prev_i][m]) / (f_max - f_min)\n",
        "\n",
        "    return dist\n",
        "\n",
        "\n",
        "def tournament_select(\n",
        "    pop_indices: List[int],\n",
        "    rank: Dict[int, int],\n",
        "    crowd: Dict[int, float],\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Binary tournament selection:\n",
        "    - pick 2 random individuals\n",
        "    - choose lower rank\n",
        "    - if tie, choose higher crowding distance\n",
        "    \"\"\"\n",
        "    a, b = np.random.choice(pop_indices, 2, replace=False)\n",
        "    if rank[a] < rank[b]:\n",
        "        return a\n",
        "    if rank[b] < rank[a]:\n",
        "        return b\n",
        "    # same rank => crowding\n",
        "    return a if crowd[a] > crowd[b] else b\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2) Evaluation Cache (speed!)\n",
        "# ============================================================\n",
        "def chrom_key(chrom) -> Tuple:\n",
        "    \"\"\"\n",
        "    Hashable key for caching chromosome evaluation.\n",
        "    We round floats so tiny noise doesn't break caching.\n",
        "    \"\"\"\n",
        "    mask_bytes = chrom.feature_mask.astype(np.uint8).tobytes()\n",
        "    return (\n",
        "        mask_bytes,\n",
        "        int(chrom.num_layers),\n",
        "        tuple(int(x) for x in chrom.hidden_units.tolist()),\n",
        "        str(chrom.activation),\n",
        "        round(float(chrom.dropout), 4),\n",
        "        round(float(chrom.lr), 8),\n",
        "        round(float(chrom.weight_decay), 8),\n",
        "    )\n",
        "\n",
        "\n",
        "def evaluate_population(\n",
        "    population,\n",
        "    cache: Dict[Tuple, Tuple[float, float, float]],\n",
        "    X_train, y_train, X_val, y_val,\n",
        "    epochs=20,\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate all chromosomes, using cache.\n",
        "    Returns list of objectives aligned with population.\n",
        "    \"\"\"\n",
        "    objs = []\n",
        "    for chrom in population:\n",
        "        k = chrom_key(chrom)\n",
        "        if k not in cache:\n",
        "            cache[k] = evaluate_chromosome(\n",
        "                chrom, X_train, y_train, X_val, y_val,\n",
        "                epochs=epochs,\n",
        "                batch_size=256,\n",
        "                min_features=5,\n",
        "            )\n",
        "        objs.append(cache[k])\n",
        "    return objs\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3) Main NSGA-II Loop\n",
        "# ============================================================\n",
        "def nsga2_optimize(\n",
        "    X_train, y_train, X_val, y_val,\n",
        "    pop_size=60,\n",
        "    generations=30,\n",
        "    crossover_prob=0.9,\n",
        "    mutation_prob=0.9,\n",
        "    eval_epochs=20,\n",
        "    seed=42,\n",
        "    verbose=True,\n",
        "):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # --- init population ---\n",
        "    n_features = X_train.shape[1]\n",
        "    population = [random_chromosome(n_features=n_features) for _ in range(pop_size)]\n",
        "    cache = {}\n",
        "\n",
        "    # --- initial eval ---\n",
        "    objs = evaluate_population(population, cache, X_train, y_train, X_val, y_val, epochs=eval_epochs)\n",
        "\n",
        "    for gen in range(1, generations + 1):\n",
        "        # ----------------------------\n",
        "        # A) Non-dominated sorting\n",
        "        # ----------------------------\n",
        "        fronts = fast_non_dominated_sort(objs)\n",
        "\n",
        "        # rank and crowding distance dict for selection\n",
        "        rank = {}\n",
        "        crowd = {}\n",
        "        for r, front in enumerate(fronts):\n",
        "            for idx in front:\n",
        "                rank[idx] = r\n",
        "            cd = crowding_distance(front, objs)\n",
        "            crowd.update(cd)\n",
        "\n",
        "        # ----------------------------\n",
        "        # B) Generate offspring\n",
        "        # ----------------------------\n",
        "        pop_indices = list(range(pop_size))\n",
        "        offspring = []\n",
        "\n",
        "        while len(offspring) < pop_size:\n",
        "            p1_idx = tournament_select(pop_indices, rank, crowd)\n",
        "            p2_idx = tournament_select(pop_indices, rank, crowd)\n",
        "            parent1 = population[p1_idx]\n",
        "            parent2 = population[p2_idx]\n",
        "\n",
        "            # crossover\n",
        "            if np.random.rand() < crossover_prob:\n",
        "                c1, c2 = crossover(parent1, parent2)\n",
        "            else:\n",
        "                c1, c2 = parent1, parent2\n",
        "\n",
        "            # mutation\n",
        "            if np.random.rand() < mutation_prob:\n",
        "                c1 = mutate(c1, p_mask=0.05, p_arch=0.25, n_features=n_features)\n",
        "            if np.random.rand() < mutation_prob:\n",
        "                c2 = mutate(c2, p_mask=0.05, p_arch=0.25, n_features=n_features)\n",
        "\n",
        "            offspring.append(c1)\n",
        "            if len(offspring) < pop_size:\n",
        "                offspring.append(c2)\n",
        "\n",
        "        offspring_objs = evaluate_population(offspring, cache, X_train, y_train, X_val, y_val, epochs=eval_epochs)\n",
        "\n",
        "        # ----------------------------\n",
        "        # C) Elitist survival selection\n",
        "        # ----------------------------\n",
        "        combined_pop = population + offspring\n",
        "        combined_objs = objs + offspring_objs\n",
        "\n",
        "        combined_fronts = fast_non_dominated_sort(combined_objs)\n",
        "\n",
        "        new_population = []\n",
        "        new_objs = []\n",
        "\n",
        "        for front in combined_fronts:\n",
        "            if len(new_population) + len(front) <= pop_size:\n",
        "                for idx in front:\n",
        "                    new_population.append(combined_pop[idx])\n",
        "                    new_objs.append(combined_objs[idx])\n",
        "            else:\n",
        "                # partial fill using crowding distance\n",
        "                cd = crowding_distance(front, combined_objs)\n",
        "                sorted_front = sorted(front, key=lambda i: cd[i], reverse=True)\n",
        "\n",
        "                remaining = pop_size - len(new_population)\n",
        "                for idx in sorted_front[:remaining]:\n",
        "                    new_population.append(combined_pop[idx])\n",
        "                    new_objs.append(combined_objs[idx])\n",
        "                break\n",
        "\n",
        "        population = new_population\n",
        "        objs = new_objs\n",
        "\n",
        "        # ----------------------------\n",
        "        # D) Logging\n",
        "        # ----------------------------\n",
        "        # Pareto front = rank 0 in current population\n",
        "        current_fronts = fast_non_dominated_sort(objs)\n",
        "        pareto = current_fronts[0]\n",
        "\n",
        "        best_rmse = min(objs[i][0] for i in pareto)\n",
        "        best_sparse = min(objs[i][2] for i in pareto)\n",
        "        best_params = min(objs[i][1] for i in pareto)\n",
        "\n",
        "        if verbose:\n",
        "            print(\n",
        "                f\"Gen {gen:03d} | Pareto size={len(pareto)} | \"\n",
        "                f\"Best RMSE={best_rmse:.4f} | Min Feats={best_sparse} | Min Params={int(best_params)}\"\n",
        "            )\n",
        "\n",
        "    # final pareto\n",
        "    final_fronts = fast_non_dominated_sort(objs)\n",
        "    pareto_idx = final_fronts[0]\n",
        "\n",
        "    pareto_solutions = [(population[i], objs[i]) for i in pareto_idx]\n",
        "    return population, objs, pareto_solutions\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4) Run NSGA-II\n",
        "# ============================================================\n",
        "# Example usage:\n",
        "# population, objs, pareto = nsga2_optimize(\n",
        "#     X_train, y_train, X_val, y_val,\n",
        "#     pop_size=60,\n",
        "#     generations=20,\n",
        "#     eval_epochs=15,\n",
        "#     verbose=True,\n",
        "# )\n",
        "#\n",
        "# print(\"\\n=== Final Pareto Solutions (first 5) ===\")\n",
        "# for chrom, (rmse, n_params, n_feats) in pareto[:5]:\n",
        "#     print(f\"RMSE={rmse:.4f}, Params={int(n_params)}, Features={n_feats}, L={chrom.num_layers}, Act={chrom.activation}\")\n"
      ],
      "metadata": {
        "id": "3B5aS1rb1i5J"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "population, objs, pareto = nsga2_optimize(\n",
        "     X_train, y_train, X_val, y_val,\n",
        "     pop_size=60,\n",
        "     generations=20,\n",
        "     eval_epochs=15,\n",
        "     verbose=True,\n",
        " )\n",
        "#\n",
        "print(\"\\n=== Final Pareto Solutions (first 5) ===\")\n",
        "for chrom, (rmse, n_params, n_feats) in pareto[:5]:\n",
        "     print(f\"RMSE={rmse:.4f}, Params={int(n_params)}, Features={n_feats}, L={chrom.num_layers}, Act={chrom.activation}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PaHvJgY1k5A",
        "outputId": "5e9dd064-fff7-423b-fcbf-2747deaf0f84"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gen 001 | Pareto size=33 | Best RMSE=13.9332 | Min Feats=8 | Min Params=239\n",
            "Gen 002 | Pareto size=42 | Best RMSE=13.6918 | Min Feats=8 | Min Params=239\n",
            "Gen 003 | Pareto size=55 | Best RMSE=13.5814 | Min Feats=8 | Min Params=239\n",
            "Gen 004 | Pareto size=60 | Best RMSE=13.1769 | Min Feats=8 | Min Params=239\n",
            "Gen 005 | Pareto size=60 | Best RMSE=13.1769 | Min Feats=8 | Min Params=239\n",
            "Gen 006 | Pareto size=60 | Best RMSE=13.1769 | Min Feats=8 | Min Params=239\n",
            "Gen 007 | Pareto size=60 | Best RMSE=13.1769 | Min Feats=8 | Min Params=239\n",
            "Gen 008 | Pareto size=60 | Best RMSE=13.1769 | Min Feats=8 | Min Params=239\n",
            "Gen 009 | Pareto size=60 | Best RMSE=13.1769 | Min Feats=8 | Min Params=239\n",
            "Gen 010 | Pareto size=60 | Best RMSE=13.1769 | Min Feats=8 | Min Params=239\n",
            "Gen 011 | Pareto size=60 | Best RMSE=13.0545 | Min Feats=8 | Min Params=222\n",
            "Gen 012 | Pareto size=60 | Best RMSE=12.7487 | Min Feats=8 | Min Params=222\n",
            "Gen 013 | Pareto size=60 | Best RMSE=12.7487 | Min Feats=8 | Min Params=222\n",
            "Gen 014 | Pareto size=60 | Best RMSE=12.7487 | Min Feats=8 | Min Params=222\n",
            "Gen 015 | Pareto size=60 | Best RMSE=12.7487 | Min Feats=8 | Min Params=222\n",
            "Gen 016 | Pareto size=60 | Best RMSE=12.7487 | Min Feats=8 | Min Params=222\n",
            "Gen 017 | Pareto size=60 | Best RMSE=12.7487 | Min Feats=8 | Min Params=205\n",
            "Gen 018 | Pareto size=60 | Best RMSE=12.7487 | Min Feats=8 | Min Params=205\n",
            "Gen 019 | Pareto size=60 | Best RMSE=12.7487 | Min Feats=8 | Min Params=205\n",
            "Gen 020 | Pareto size=60 | Best RMSE=12.7487 | Min Feats=8 | Min Params=171\n",
            "\n",
            "=== Final Pareto Solutions (first 5) ===\n",
            "RMSE=15.2870, Params=371073, Features=8, L=4, Act=relu\n",
            "RMSE=12.7487, Params=486187, Features=31, L=4, Act=relu\n",
            "RMSE=16.8781, Params=3241, Features=38, L=1, Act=leaky_relu\n",
            "RMSE=13.4417, Params=493388, Features=18, L=4, Act=relu\n",
            "RMSE=29.5555, Params=171, Features=8, L=1, Act=relu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def random_search(\n",
        "    X_train, y_train, X_val, y_val,\n",
        "    n_trials=200,\n",
        "    eval_epochs=20,\n",
        "    seed=42,\n",
        "    verbose=True,\n",
        "):\n",
        "    np.random.seed(seed)\n",
        "    n_features = X_train.shape[1]\n",
        "\n",
        "    results = []  # (chrom, (rmse, params, feats))\n",
        "\n",
        "    for t in range(1, n_trials + 1):\n",
        "        chrom = random_chromosome(n_features=n_features)\n",
        "\n",
        "        obj = evaluate_chromosome(\n",
        "            chrom,\n",
        "            X_train, y_train,\n",
        "            X_val, y_val,\n",
        "            epochs=eval_epochs,\n",
        "            batch_size=256,\n",
        "            min_features=5,\n",
        "        )\n",
        "\n",
        "        results.append((chrom, obj))\n",
        "\n",
        "        if verbose and t % 20 == 0:\n",
        "            rmse, params, feats = obj\n",
        "            print(f\"[RandomSearch] Trial {t:04d}/{n_trials} | RMSE={rmse:.4f} | Params={int(params)} | Feats={feats}\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "xh9RrMAr7Pks"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rs_results = random_search(\n",
        "    X_train, y_train, X_val, y_val,\n",
        "    n_trials=200,\n",
        "    eval_epochs=15\n",
        ")\n",
        "\n",
        "# Best by RMSE (single-objective view)\n",
        "best = min(rs_results, key=lambda x: x[1][0])\n",
        "print(\"Best RandomSearch RMSE:\", best[1][0], \"Features:\", best[1][2], \"Params:\", int(best[1][1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYsqPV_eBLFu",
        "outputId": "67d3d5fc-34e3-4309-9fcd-3ee25c40be38"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RandomSearch] Trial 0020/200 | RMSE=21.8696 | Params=239 | Feats=12\n",
            "[RandomSearch] Trial 0040/200 | RMSE=37.2296 | Params=1081 | Feats=28\n",
            "[RandomSearch] Trial 0060/200 | RMSE=15.6968 | Params=41690 | Feats=33\n",
            "[RandomSearch] Trial 0080/200 | RMSE=15.2213 | Params=135464 | Feats=25\n",
            "[RandomSearch] Trial 0100/200 | RMSE=14.9517 | Params=242958 | Feats=23\n",
            "[RandomSearch] Trial 0120/200 | RMSE=17.0529 | Params=91254 | Feats=10\n",
            "[RandomSearch] Trial 0140/200 | RMSE=18.9131 | Params=13295 | Feats=32\n",
            "[RandomSearch] Trial 0160/200 | RMSE=21.4045 | Params=273863 | Feats=20\n",
            "[RandomSearch] Trial 0180/200 | RMSE=40.9771 | Params=29439 | Feats=37\n",
            "[RandomSearch] Trial 0200/200 | RMSE=31.8832 | Params=2738 | Feats=21\n",
            "Best RandomSearch RMSE: 13.801971980191533 Features: 26 Params: 234034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def make_chromosome(\n",
        "    n_total_features: int,\n",
        "    k_features: int,\n",
        "    num_layers: int,\n",
        "    hidden_units: list,\n",
        "    activation: str,\n",
        "    dropout: float = 0.1,\n",
        "    lr: float = 1e-3,\n",
        "    weight_decay: float = 1e-5,\n",
        ") -> Chromosome:\n",
        "    \"\"\"\n",
        "    hidden_units must be length 4 (we only use first num_layers)\n",
        "    \"\"\"\n",
        "    # Randomly pick k features (wrapper-style)\n",
        "    mask = np.zeros(n_total_features, dtype=np.int32)\n",
        "    idx = np.random.choice(n_total_features, size=k_features, replace=False)\n",
        "    mask[idx] = 1\n",
        "\n",
        "    hu = np.array(hidden_units, dtype=np.int32)\n",
        "    assert hu.shape[0] == 4, \"hidden_units must have length 4\"\n",
        "\n",
        "    return Chromosome(\n",
        "        feature_mask=mask,\n",
        "        num_layers=int(num_layers),\n",
        "        hidden_units=hu,\n",
        "        activation=str(activation),\n",
        "        dropout=float(dropout),\n",
        "        lr=float(lr),\n",
        "        weight_decay=float(weight_decay),\n",
        "    )\n"
      ],
      "metadata": {
        "id": "lzuDKlVrx_6h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "def grid_search(\n",
        "    X_train, y_train, X_val, y_val,\n",
        "    k_feature_grid=(8, 12, 20, 30, 40, 60, 81),\n",
        "    layer_grid=(1, 2, 3),\n",
        "    width_grid=((64,), (128,), (256,), (256,128), (256,128,64)),\n",
        "    activation_grid=(\"relu\", \"gelu\"),\n",
        "    repeats_per_setting=2,        # repeat each grid point with different random feature subsets\n",
        "    eval_epochs=20,\n",
        "    seed=42,\n",
        "    verbose=True,\n",
        "):\n",
        "    np.random.seed(seed)\n",
        "    n_total_features = X_train.shape[1]\n",
        "\n",
        "    results = []  # (chrom, (rmse, params, feats))\n",
        "\n",
        "    # Convert widths into 4-length hidden_units arrays\n",
        "    def pad_widths(widths):\n",
        "        padded = list(widths) + [16] * (4 - len(widths))  # fillers won't be used if num_layers < len(widths)\n",
        "        return padded[:4]\n",
        "\n",
        "    grid = list(itertools.product(k_feature_grid, layer_grid, width_grid, activation_grid))\n",
        "\n",
        "    total_trials = len(grid) * repeats_per_setting\n",
        "    trial = 0\n",
        "\n",
        "    for (k, L, widths, act) in grid:\n",
        "        # skip invalid combos (can't have L > len(widths))\n",
        "        if L > len(widths):\n",
        "            continue\n",
        "\n",
        "        hidden_4 = pad_widths(widths)\n",
        "\n",
        "        for _ in range(repeats_per_setting):\n",
        "            trial += 1\n",
        "\n",
        "            chrom = make_chromosome(\n",
        "                n_total_features=n_total_features,\n",
        "                k_features=int(k),\n",
        "                num_layers=int(L),\n",
        "                hidden_units=hidden_4,\n",
        "                activation=act,\n",
        "                dropout=0.1,\n",
        "                lr=1e-3,\n",
        "                weight_decay=1e-5\n",
        "            )\n",
        "\n",
        "            obj = evaluate_chromosome(\n",
        "                chrom,\n",
        "                X_train, y_train,\n",
        "                X_val, y_val,\n",
        "                epochs=eval_epochs,\n",
        "                batch_size=256,\n",
        "                min_features=5,\n",
        "            )\n",
        "\n",
        "            results.append((chrom, obj))\n",
        "\n",
        "            if verbose and trial % 20 == 0:\n",
        "                rmse, params, feats = obj\n",
        "                print(f\"[GridSearch] Trial {trial:04d}/{total_trials} | RMSE={rmse:.4f} | Params={int(params)} | Feats={feats}\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "YIKIn1MXxO3f"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gs_results = grid_search(\n",
        "    X_train, y_train, X_val, y_val,\n",
        "    k_feature_grid=(8, 12, 20, 40, 81),\n",
        "    layer_grid=(1, 2, 3),\n",
        "    width_grid=((64,), (128,), (256,), (256,128), (256,128,64)),\n",
        "    activation_grid=(\"relu\", \"gelu\"),\n",
        "    repeats_per_setting=2,\n",
        "    eval_epochs=15\n",
        ")\n",
        "\n",
        "best_gs = min(gs_results, key=lambda x: x[1][0])\n",
        "print(\"Best GridSearch RMSE:\", best_gs[1][0], \"Features:\", best_gs[1][2], \"Params:\", int(best_gs[1][1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0Gw9l5FxYMy",
        "outputId": "bab4340b-0ba8-4050-997d-da00882dc5fd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GridSearch] Trial 0020/300 | RMSE=21.3228 | Params=2561 | Feats=8\n",
            "[GridSearch] Trial 0040/300 | RMSE=20.7464 | Params=1793 | Feats=12\n",
            "[GridSearch] Trial 0060/300 | RMSE=17.9287 | Params=36353 | Feats=12\n",
            "[GridSearch] Trial 0080/300 | RMSE=18.2716 | Params=5633 | Feats=20\n",
            "[GridSearch] Trial 0100/300 | RMSE=17.1573 | Params=2689 | Feats=40\n",
            "[GridSearch] Trial 0120/300 | RMSE=15.2492 | Params=43521 | Feats=40\n",
            "[GridSearch] Trial 0140/300 | RMSE=15.3428 | Params=21249 | Feats=81\n",
            "[GridSearch] Trial 0160/300 | RMSE=13.9287 | Params=62209 | Feats=81\n",
            "Best GridSearch RMSE: 13.92567003374243 Features: 81 Params: 62209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "H4KXAUbIwKMd"
      }
    }
  ]
}